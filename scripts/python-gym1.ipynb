{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "action_column_size = env.action_space.n\n",
    "observation_column_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((observation_column_size, action_column_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "num_ep = 10000\n",
    "num_step = 100\n",
    "\n",
    "discount_rate = 0.99\n",
    "learning_rate = 0.1\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exp_rate = 1\n",
    "min_exp_rate = 0.01\n",
    "exp_rate_decay = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Average reward per thousand episodes-------------\n",
      "\n",
      "1000 :  0.05000000000000004\n",
      "2000 :  0.20000000000000015\n",
      "3000 :  0.3800000000000003\n",
      "4000 :  0.5480000000000004\n",
      "5000 :  0.6280000000000004\n",
      "6000 :  0.6710000000000005\n",
      "7000 :  0.6720000000000005\n",
      "8000 :  0.6770000000000005\n",
      "9000 :  0.7070000000000005\n",
      "10000 :  0.6560000000000005\n"
     ]
    }
   ],
   "source": [
    "all_ep_reward = []\n",
    "\n",
    "for episode in range(num_ep):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for step in range(num_step):\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "                    learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    exploration_rate = min_exp_rate + \\\n",
    "        (max_exp_rate - min_exp_rate) * np.exp(-exp_rate_decay*episode)\n",
    "    \n",
    "    all_ep_reward.append(current_ep_reward)\n",
    "\n",
    "rewards_per_thousand_episodes = np.split(np.array(all_ep_reward),num_ep/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"-------------Average reward per thousand episodes-------------\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "the first loop initialises the state, this is the reset of the environment each different episode.\n",
    "the done boolean is used to flag when the end of the episode is reached\n",
    "\n",
    "the second loop is used for each individual time step process the exploration and exploitation balance is calculated\n",
    "the selection statement thereby deciding between choosing exploitation or exploration\n",
    "\n",
    "if the generated random value is greater than the exploration rate, then the exploit task is run, this finds the greatest\n",
    "value inside the q table and chooses that as the action\n",
    "\n",
    "the exploration action is a random action chosen from the list of possible actions to take inside the state\n",
    "\n",
    "the time step using the action will generate a tuple containing the new state generated, the reward, a done boolean value\n",
    "and info for debugging the environment\n",
    "\n",
    "the q table can now be updated with the current state action pair using the learning rate defined previously\n",
    "\n",
    "the future state is assigned to the state returned from taking action and\n",
    "the current episode reward is updated\n",
    "\n",
    "if the done boolean was assigned true, break the time step loop\n",
    "\n",
    "then the exploration rate can be decreased and the total episodic reward array is appended with the episodes total reward\n",
    "\n",
    "all the episodes reward data is output per thousand episodes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48803581 0.47789319 0.470883   0.47873923]\n",
      " [0.30647229 0.41358042 0.30214483 0.44838582]\n",
      " [0.38545966 0.40153945 0.39630651 0.42380083]\n",
      " [0.34521508 0.30609306 0.27980109 0.4070392 ]\n",
      " [0.50296345 0.4239096  0.28752496 0.33595976]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28749609 0.14615329 0.20425708 0.12189876]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.39615814 0.43210045 0.39091532 0.55955479]\n",
      " [0.42796624 0.63144265 0.35097213 0.27474072]\n",
      " [0.65183738 0.31521139 0.31731571 0.2494679 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.54381314 0.58028561 0.75648961 0.4135029 ]\n",
      " [0.72761503 0.88017202 0.73017106 0.70790141]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "this q table shows the q values on each state action pair\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitd0b078c581744c51ba9098f8d851824d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}